{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "735185c0-8cfa-4e84-8348-41b32cae8b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Beam Search...\n",
      "[Step 1] S=[6] added_t=6 MAE_train=0.074042 MSE_train=0.083836 R2_train=0.754327 | MAE_val=0.070176 MSE_val=0.082016 R2_val=0.705493 time=41.706s\n",
      "[Step 2] S=[6, 0] added_t=0 MAE_train=0.046605 MSE_train=0.033404 R2_train=0.902101 | MAE_val=0.046652 MSE_val=0.034845 R2_val=0.870695 time=499.206s\n",
      "[Step 3] S=[0, 3, 6] added_t=6 MAE_train=0.036371 MSE_train=0.019087 R2_train=0.943127 | MAE_val=0.039161 MSE_val=0.024394 R2_val=0.910364 time=1892.235s\n",
      "[Step 4] S=[4, 0, 6, 5] added_t=5 MAE_train=0.031413 MSE_train=0.017745 R2_train=0.946225 | MAE_val=0.033447 MSE_val=0.020655 R2_val=0.922659 time=1473.084s\n",
      "[Step 5] S=[5, 6, 0, 2, 3] added_t=3 MAE_train=0.024092 MSE_train=0.011499 R2_train=0.964533 | MAE_val=0.027391 MSE_val=0.015175 R2_val=0.947022 time=1087.636s\n",
      "[Step 6] S=[5, 0, 6, 2, 3, 4] added_t=4 MAE_train=0.016372 MSE_train=0.008374 R2_train=0.972686 | MAE_val=0.018590 MSE_val=0.012695 R2_val=0.956075 time=725.940s\n",
      "[Step 7] S=[6, 4, 0, 2, 5, 3, 1] added_t=1 MAE_train=0.022206 MSE_train=0.007233 R2_train=0.979588 | MAE_val=0.025475 MSE_val=0.009802 R2_val=0.965334 time=368.025s\n",
      "\n",
      "Selected S: [6, 4, 0, 2, 5, 3, 1]\n",
      "Prediction shape: (6000, 7)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# === DTPSP-style reference time-point selection — single-cell-friendly\n",
    "# Version with PE_S removed completely.\n",
    "# Encapsulated Version with Normalization Flag and Fix for AttributeError\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import math, random\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ------------------ Helpers ------------------\n",
    "def _fmt_secs(sec):\n",
    "    if sec < 1e-3: return f\"{sec*1e6:.1f}µs\"\n",
    "    if sec < 1.0: return f\"{sec*1e3:.1f}ms\"\n",
    "    return f\"{sec:.3f}s\"\n",
    "\n",
    "def set_seed(s, device_str=\"cpu\"):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "    if \"cuda\" in device_str:\n",
    "        torch.cuda.manual_seed_all(s)\n",
    "\n",
    "def sinusoidal_pe(t_idx: np.ndarray, d_model: int):\n",
    "    pe = np.zeros((len(t_idx), d_model), dtype=np.float32)\n",
    "    pos = t_idx[:, None].astype(np.float32)\n",
    "    div = np.exp(np.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = np.sin(pos * div)\n",
    "    pe[:, 1::2] = np.cos(pos * div)\n",
    "    return pe\n",
    "\n",
    "def _cosine_knn_graph(X_GT, k, device):\n",
    "    # X_GT is (G, T) or similar features per gene\n",
    "    Xn = X_GT / (np.linalg.norm(X_GT, axis=1, keepdims=True) + 1e-12)\n",
    "    S_mat = Xn @ Xn.T\n",
    "    np.fill_diagonal(S_mat, 0)\n",
    "    rows, cols, vals = [], [], []\n",
    "    G_ = X_GT.shape[0]\n",
    "    for g in range(G_):\n",
    "        idx = np.argpartition(-S_mat[g], k-1)[:k]\n",
    "        for j in idx:\n",
    "            rows.append(g); cols.append(j); vals.append(S_mat[g, j])\n",
    "    rows.extend(range(G_))\n",
    "    cols.extend(range(G_))\n",
    "    vals.extend([1.0]*G_)\n",
    "    edge_i = torch.tensor([rows, cols], dtype=torch.long, device=device)\n",
    "    edge_v = torch.tensor(vals, dtype=torch.float32, device=device)\n",
    "    deg = torch.zeros(G_, device=device).scatter_add_(0, edge_i[0], edge_v)\n",
    "    dinv = deg.pow(-0.5); dinv[deg==0] = 0\n",
    "    edge_v = dinv[edge_i[0]] * edge_v * dinv[edge_i[1]]\n",
    "    return torch.sparse_coo_tensor(edge_i, edge_v, (G_, G_)).coalesce()\n",
    "\n",
    "\n",
    "# ------------------ Model Classes ------------------\n",
    "class TinyAE(nn.Module):\n",
    "    def __init__(self, d_in: int, hidden_dim: int, d_latent: int = 64):\n",
    "        super().__init__()\n",
    "        self.d_latent = d_latent  # <--- FIXED: Added missing attribute\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(d_in, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, d_latent), nn.ReLU()\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(d_latent, hidden_dim), nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, d_in)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        rec = self.dec(z)\n",
    "        return rec, z\n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, d_in, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(d_in, hidden_dim), nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n",
    "        )\n",
    "        self.head = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x))\n",
    "\n",
    "class GraphConv(nn.Module):\n",
    "    def __init__(self, din, dout, adj_matrix):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(din, dout, bias=False)\n",
    "        self.adj = adj_matrix \n",
    "    def forward(self, x):\n",
    "        return torch.relu(torch.sparse.mm(self.adj, self.lin(x)))\n",
    "\n",
    "class GeneGCN(nn.Module):\n",
    "    def __init__(self, t_in, gcn_dim1, gcn_dim2, adj_matrix):\n",
    "        super().__init__()\n",
    "        self.gc1 = GraphConv(t_in, gcn_dim1, adj_matrix)\n",
    "        self.gc2 = GraphConv(gcn_dim1, gcn_dim2, adj_matrix)\n",
    "    def forward(self, X_mask):\n",
    "        return self.gc2(self.gc1(X_mask))\n",
    "\n",
    "\n",
    "# ------------------ Data Utilities ------------------\n",
    "class ReconDatasetMaskedFullT(torch.utils.data.Dataset):\n",
    "    def __init__(self, X_log, S, gene_idx, T_total):\n",
    "        super().__init__()\n",
    "        self.S = np.array(S)\n",
    "        self.gene_idx = np.array(gene_idx)\n",
    "        X_full = X_log.T.astype(np.float32)\n",
    "        mask = np.zeros(T_total, dtype=np.float32); mask[self.S] = 1\n",
    "        self.X_masked = X_full * mask[None,:]\n",
    "        self.X_target = X_full\n",
    "    def __len__(self):\n",
    "        return len(self.gene_idx)\n",
    "    def __getitem__(self, idx):\n",
    "        g = self.gene_idx[idx]\n",
    "        return (torch.from_numpy(self.X_masked[g]), torch.from_numpy(self.X_target[g]))\n",
    "\n",
    "class PairSampler(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, genes_all, target_times_all, S, X_log, ae, gcn, z_gcn_full, batch_g, require_grad, device, pe_dim, T_total):\n",
    "        super().__init__()\n",
    "        self.genes_all = genes_all\n",
    "        self.target_times_all = target_times_all\n",
    "        self.S = list(S)\n",
    "        self.X_log = X_log\n",
    "        self.ae = ae\n",
    "        self.gcn = gcn\n",
    "        self.z_gcn_full = z_gcn_full\n",
    "        self.batch_g = batch_g\n",
    "        self.require_grad = require_grad\n",
    "        self.N = len(genes_all)\n",
    "        self.device = device\n",
    "        self.pe_dim = pe_dim\n",
    "        self.T_total = T_total\n",
    "\n",
    "    def __iter__(self):\n",
    "        order = np.arange(self.N); np.random.shuffle(order)\n",
    "        for i in range(0, self.N, self.batch_g):\n",
    "            idx = order[i:i+self.batch_g]\n",
    "            g_batch = self.genes_all[idx]\n",
    "            t_batch = self.target_times_all[idx]\n",
    "            \n",
    "            S_arr = np.array(self.S, dtype=np.int64)\n",
    "            X_full = self.X_log.T.astype(np.float32)\n",
    "            mask = np.zeros(self.T_total, dtype=np.float32); mask[S_arr] = 1\n",
    "            X_mask = X_full * mask[None, :]\n",
    "\n",
    "            gv = X_mask[g_batch].astype(np.float32)\n",
    "            gv_t = torch.from_numpy(gv).to(self.device)\n",
    "\n",
    "            if self.require_grad:\n",
    "                z_ae = self.ae.enc(gv_t)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    z_ae = self.ae.enc(gv_t)\n",
    "\n",
    "            if self.z_gcn_full is not None:\n",
    "                z_gcn = self.z_gcn_full[torch.from_numpy(g_batch).to(self.device)]\n",
    "            else:\n",
    "                X_G_T_masked = torch.from_numpy(X_mask).to(self.device)\n",
    "                with torch.set_grad_enabled(self.require_grad):\n",
    "                    z_all = self.gcn(X_G_T_masked)\n",
    "                z_gcn = z_all[torch.from_numpy(g_batch).to(self.device)]\n",
    "\n",
    "            pe_t_np = sinusoidal_pe(t_batch, d_model=self.pe_dim)\n",
    "            pe_t = torch.from_numpy(pe_t_np).to(self.device)\n",
    "\n",
    "            Xb = torch.cat([z_ae, z_gcn, pe_t], dim=1)\n",
    "            yb = torch.from_numpy(\n",
    "                self.X_log[t_batch, g_batch].astype(np.float32)[:,None]\n",
    "            ).to(self.device)\n",
    "            yield Xb, yb\n",
    "\n",
    "\n",
    "# ------------------ Main Class ------------------\n",
    "class DTPSP_Selector:\n",
    "    def __init__(self, \n",
    "                 seed=1234, \n",
    "                 device=None,\n",
    "                 max_ref=15,\n",
    "                 train_frac=0.8,\n",
    "                 shuffle_split=True,\n",
    "                 ae_epochs=10,\n",
    "                 reg_epochs=10,\n",
    "                 ft_epochs=10,\n",
    "                 batch_g=128,\n",
    "                 lr=1e-3,\n",
    "                 ft_lr_factor=0.2,\n",
    "                 lambda_r=1.0,\n",
    "                 hidden=128,\n",
    "                 dropout=0.0,\n",
    "                 pe_dim=16,\n",
    "                 k_neighb=30,\n",
    "                 gcn_dim1=128,\n",
    "                 gcn_dim2=64,\n",
    "                 target_lib=1e6,\n",
    "                 beam_width=32,\n",
    "                 precompute_gcn=False):\n",
    "        \n",
    "        self.SEED = seed\n",
    "        self.DEVICE = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.MAX_REF = max_ref\n",
    "        self.TRAIN_FRAC = train_frac\n",
    "        self.SHUFFLE_SPLIT = shuffle_split\n",
    "        \n",
    "        self.AE_EPOCHS = ae_epochs\n",
    "        self.REG_EPOCHS = reg_epochs\n",
    "        self.FT_EPOCHS = ft_epochs\n",
    "        self.BATCH_G = batch_g\n",
    "        self.LR = lr\n",
    "        self.FT_LR_FACTOR = ft_lr_factor\n",
    "        self.LAMBDA_R = lambda_r\n",
    "        \n",
    "        self.HIDDEN = hidden\n",
    "        self.DROPOUT = dropout\n",
    "        self.PE_DIM = pe_dim\n",
    "        \n",
    "        self.K_NEIGHB = k_neighb\n",
    "        self.GCN_DIM1 = gcn_dim1\n",
    "        self.GCN_DIM2 = gcn_dim2\n",
    "        \n",
    "        self.TARGET_LIB = target_lib\n",
    "        self.BEAM_WIDTH = beam_width\n",
    "        self.PRECOMPUTE_GCN = precompute_gcn\n",
    "        \n",
    "        set_seed(self.SEED, self.DEVICE)\n",
    "\n",
    "    def _normalize(self, adata):\n",
    "        if self.TARGET_LIB is not None:\n",
    "            sc.pp.normalize_total(adata, target_sum=self.TARGET_LIB)\n",
    "        sc.pp.log1p(adata)\n",
    "        return adata\n",
    "\n",
    "    def build_gene_masked_fullT_tensor(self, X_log, S):\n",
    "        T_total, G_total = X_log.shape\n",
    "        X_full = X_log.T.astype(np.float32)\n",
    "        mask = np.zeros(T_total, dtype=np.float32)\n",
    "        mask[np.array(S, dtype=np.int64)] = 1.0\n",
    "        X_mask = X_full * mask[None, :]\n",
    "        return torch.from_numpy(X_mask).to(self.DEVICE)\n",
    "\n",
    "    def fit(self, adata_input: ad.AnnData, normalize_data: bool = False, verbose: bool = True):\n",
    "        set_seed(self.SEED, self.DEVICE)\n",
    "        \n",
    "        if normalize_data:\n",
    "            adata = self._normalize(adata_input.copy())\n",
    "        else:\n",
    "            adata = adata_input\n",
    "\n",
    "        X0 = adata.X.A if hasattr(adata.X, \"A\") else np.array(adata.X, dtype=np.float32)\n",
    "        \n",
    "        T_obs, G_var = adata.n_obs, adata.n_vars\n",
    "        if X0.shape == (T_obs, G_var):\n",
    "            X_raw = X0.astype(np.float32)\n",
    "        elif X0.shape == (G_var, T_obs):\n",
    "            X_raw = X0.T.astype(np.float32)\n",
    "        else:\n",
    "            X_raw = (X0 if X0.shape[0] <= X0.shape[1] else X0.T).astype(np.float32)\n",
    "\n",
    "        self.X_log = X_raw\n",
    "        self.T, self.G = self.X_log.shape\n",
    "        self.times = np.arange(self.T, dtype=np.int64)\n",
    "\n",
    "        perm = np.random.permutation(self.G) if self.SHUFFLE_SPLIT else np.arange(self.G)\n",
    "        n_train = max(1, int(round(self.TRAIN_FRAC * self.G)))\n",
    "        if n_train >= self.G: n_train = self.G - 1\n",
    "        genes_train = perm[:n_train]\n",
    "        genes_val = perm[n_train:]\n",
    "        if len(genes_val) == 0:\n",
    "            genes_val = perm[-1:]\n",
    "            genes_train = perm[:-1]\n",
    "            \n",
    "        self.genes_train = genes_train\n",
    "        self.genes_val = genes_val\n",
    "\n",
    "        self.A_NORM = _cosine_knn_graph(self.X_log.T, self.K_NEIGHB, self.DEVICE)\n",
    "\n",
    "        S_sel, pack_sel, hist = self.beam_search_select(\n",
    "            beam_width=self.BEAM_WIDTH,\n",
    "            max_ref=self.MAX_REF,\n",
    "            genes_train_in=self.genes_train,\n",
    "            genes_val_in=self.genes_val,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        return S_sel, pack_sel, hist\n",
    "\n",
    "    def make_pairs(self, S, genes_all, use_all_targets=True, n_targets_per_gene=None):\n",
    "        remaining = np.setdiff1d(self.times, np.array(S))\n",
    "        if len(remaining) == 0:\n",
    "            remaining = self.times.copy()\n",
    "        if use_all_targets:\n",
    "            t_list = np.repeat(remaining, len(genes_all))\n",
    "            g_list = np.tile(genes_all, len(remaining))\n",
    "        else:\n",
    "            n = int(n_targets_per_gene or 1)\n",
    "            choices = np.random.choice(remaining, size=(len(genes_all), n), replace=True)\n",
    "            g_list = np.repeat(genes_all, n)\n",
    "            t_list = choices.ravel()\n",
    "        return g_list.astype(np.int64), t_list.astype(np.int64)\n",
    "\n",
    "    def pretrain_ae_stage1(self, S, gene_idx=None, d_latent=64):\n",
    "        assert len(S) >= 1\n",
    "        if gene_idx is None: gene_idx = self.genes_train\n",
    "        ds = ReconDatasetMaskedFullT(self.X_log, S, gene_idx, self.T)\n",
    "        dl = torch.utils.data.DataLoader(ds, batch_size=2048, shuffle=True)\n",
    "        \n",
    "        ae = TinyAE(self.T, self.HIDDEN, d_latent).to(self.DEVICE)\n",
    "        opt = optim.Adam(ae.parameters(), lr=self.LR)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        S_idx = torch.tensor(S, dtype=torch.long, device=self.DEVICE)\n",
    "\n",
    "        ae.train()\n",
    "        for _ in range(self.AE_EPOCHS):\n",
    "            for xb, yb in dl:\n",
    "                xb = xb.to(self.DEVICE)\n",
    "                yb = yb.to(self.DEVICE)\n",
    "                rec, _ = ae(xb)\n",
    "                loss = loss_fn(\n",
    "                    rec.index_select(1, S_idx),\n",
    "                    yb.index_select(1, S_idx)\n",
    "                )\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "        return ae\n",
    "\n",
    "    def stage2_train_regressor(self, S, train_pairs, val_pairs, ae):\n",
    "        g_tr, t_tr = train_pairs\n",
    "        g_va, t_va = val_pairs\n",
    "\n",
    "        gcn = GeneGCN(self.T, self.GCN_DIM1, self.GCN_DIM2, self.A_NORM).to(self.DEVICE)\n",
    "\n",
    "        if self.PRECOMPUTE_GCN:\n",
    "            for p in gcn.parameters(): p.requires_grad = False\n",
    "            with torch.no_grad():\n",
    "                X_mask = self.build_gene_masked_fullT_tensor(self.X_log, S)\n",
    "                z_gcn_full = gcn(X_mask)\n",
    "        else:\n",
    "            z_gcn_full = None\n",
    "\n",
    "        d_in = ae.d_latent + self.GCN_DIM2 + self.PE_DIM\n",
    "        reg = Regressor(d_in, self.HIDDEN, self.DROPOUT).to(self.DEVICE)\n",
    "\n",
    "        for p in ae.parameters(): p.requires_grad = False\n",
    "\n",
    "        params = list(reg.parameters())\n",
    "        if not self.PRECOMPUTE_GCN:\n",
    "            params += list(gcn.parameters())\n",
    "        opt = optim.Adam(params, lr=self.LR)\n",
    "        loss_fn = nn.L1Loss()\n",
    "\n",
    "        tr_ds = PairSampler(g_tr, t_tr, S, self.X_log, ae, gcn, z_gcn_full, self.BATCH_G, True, self.DEVICE, self.PE_DIM, self.T)\n",
    "        va_ds = PairSampler(g_va, t_va, S, self.X_log, ae, gcn, z_gcn_full, self.BATCH_G, False, self.DEVICE, self.PE_DIM, self.T)\n",
    "        tr_dl = torch.utils.data.DataLoader(tr_ds, batch_size=None)\n",
    "        va_dl = torch.utils.data.DataLoader(va_ds, batch_size=None)\n",
    "\n",
    "        reg.train()\n",
    "        gcn.train() if (not self.PRECOMPUTE_GCN) else gcn.eval()\n",
    "        for _ in range(self.REG_EPOCHS):\n",
    "            for xb, yb in tr_dl:\n",
    "                pred = reg(xb)\n",
    "                loss = loss_fn(pred, yb)\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "        reg.eval(); gcn.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in va_dl:\n",
    "                yp = reg(xb)\n",
    "                y_true.append(yb.cpu().numpy().ravel())\n",
    "                y_pred.append(yp.cpu().numpy().ravel())\n",
    "        y_true = np.hstack(y_true)\n",
    "        y_pred = np.hstack(y_pred)\n",
    "        mae = float(np.mean(np.abs(y_pred - y_true)))\n",
    "        y_bar = float(np.mean(y_true))\n",
    "        sst = float(np.sum((y_true - y_bar)**2))\n",
    "        R2 = float(\"nan\") if sst <= 0 else 1 - (np.sum((y_pred - y_true)**2) / sst)\n",
    "        return reg, gcn, z_gcn_full, mae, R2\n",
    "\n",
    "    def stage3_joint_finetune(self, S, train_pairs, val_pairs, ae, reg, gcn, z_gcn_full):\n",
    "        lr_ft = self.LR * self.FT_LR_FACTOR\n",
    "        for p in ae.parameters(): p.requires_grad = True\n",
    "\n",
    "        if self.PRECOMPUTE_GCN:\n",
    "            for p in gcn.parameters(): p.requires_grad = False\n",
    "            params = list(ae.parameters()) + list(reg.parameters())\n",
    "        else:\n",
    "            params = list(ae.parameters()) + list(reg.parameters()) + list(gcn.parameters())\n",
    "        opt = optim.Adam(params, lr=lr_ft)\n",
    "        loss_rec = nn.MSELoss()\n",
    "        loss_reg = nn.L1Loss()\n",
    "\n",
    "        recon_ds = ReconDatasetMaskedFullT(self.X_log, S, self.genes_train, self.T)\n",
    "        recon_dl = torch.utils.data.DataLoader(recon_ds, batch_size=2048, shuffle=True)\n",
    "        S_idx = torch.tensor(S, dtype=torch.long, device=self.DEVICE)\n",
    "\n",
    "        g_tr, t_tr = train_pairs\n",
    "        g_va, t_va = val_pairs\n",
    "        tr_ds = PairSampler(g_tr, t_tr, S, self.X_log, ae, gcn, z_gcn_full, self.BATCH_G, True, self.DEVICE, self.PE_DIM, self.T)\n",
    "        va_ds = PairSampler(g_va, t_va, S, self.X_log, ae, gcn, z_gcn_full, self.BATCH_G, False, self.DEVICE, self.PE_DIM, self.T)\n",
    "        tr_dl = torch.utils.data.DataLoader(tr_ds, batch_size=None)\n",
    "        va_dl = torch.utils.data.DataLoader(va_ds, batch_size=None)\n",
    "\n",
    "        for _ in range(self.FT_EPOCHS):\n",
    "            ae.train(); reg.train()\n",
    "            gcn.eval() if self.PRECOMPUTE_GCN else gcn.train()\n",
    "            recon_iter = iter(recon_dl)\n",
    "\n",
    "            for xb, yb in tr_dl:\n",
    "                pred = reg(xb)\n",
    "                L_R = loss_reg(pred, yb)\n",
    "\n",
    "                try:\n",
    "                    xr, yr = next(recon_iter)\n",
    "                except StopIteration:\n",
    "                    recon_iter = iter(recon_dl)\n",
    "                    xr, yr = next(recon_iter)\n",
    "                xr = xr.to(self.DEVICE); yr = yr.to(self.DEVICE)\n",
    "                rec, _ = ae(xr)\n",
    "                L_AE = loss_rec(\n",
    "                    rec.index_select(1, S_idx),\n",
    "                    yr.index_select(1, S_idx)\n",
    "                )\n",
    "\n",
    "                L = L_AE + self.LAMBDA_R * L_R\n",
    "                opt.zero_grad()\n",
    "                L.backward()\n",
    "                opt.step()\n",
    "\n",
    "        # ---- metrics ----\n",
    "        reg.eval(); gcn.eval()\n",
    "        \n",
    "        tr_ds2 = PairSampler(g_tr, t_tr, S, self.X_log, ae, gcn, z_gcn_full, self.BATCH_G, False, self.DEVICE, self.PE_DIM, self.T)\n",
    "        tr_dl2 = torch.utils.data.DataLoader(tr_ds2, batch_size=None)\n",
    "        with torch.no_grad():\n",
    "            y_true_tr, y_pred_tr = [], []\n",
    "            for xb, yb in tr_dl2:\n",
    "                yp = reg(xb)\n",
    "                y_true_tr.append(yb.cpu().numpy().ravel())\n",
    "                y_pred_tr.append(yp.cpu().numpy().ravel())\n",
    "        y_true_tr = np.hstack(y_true_tr)\n",
    "        y_pred_tr = np.hstack(y_pred_tr)\n",
    "        mae_tr = float(np.mean(np.abs(y_pred_tr - y_true_tr)))\n",
    "        mse_tr = float(np.mean((y_pred_tr - y_true_tr)**2))\n",
    "        y_bar = float(np.mean(y_true_tr))\n",
    "        sst = np.sum((y_true_tr - y_bar)**2)\n",
    "        R2_tr = float(\"nan\") if sst <= 0 else 1 - (np.sum((y_pred_tr - y_true_tr)**2) / sst)\n",
    "\n",
    "        va_ds2 = PairSampler(g_va, t_va, S, self.X_log, ae, gcn, z_gcn_full, self.BATCH_G, False, self.DEVICE, self.PE_DIM, self.T)\n",
    "        va_dl2 = torch.utils.data.DataLoader(va_ds2, batch_size=None)\n",
    "        with torch.no_grad():\n",
    "            y_true_va, y_pred_va = [], []\n",
    "            for xb, yb in va_dl2:\n",
    "                yp = reg(xb)\n",
    "                y_true_va.append(yb.cpu().numpy().ravel())\n",
    "                y_pred_va.append(yp.cpu().numpy().ravel())\n",
    "        y_true_va = np.hstack(y_true_va)\n",
    "        y_pred_va = np.hstack(y_pred_va)\n",
    "        mae_va = float(np.mean(np.abs(y_true_va - y_pred_va)))\n",
    "        mse_va = float(np.mean((y_true_va - y_pred_va)**2))\n",
    "        y_bar = float(np.mean(y_true_va))\n",
    "        sst = np.sum((y_true_va - y_bar)**2)\n",
    "        R2_va = float(\"nan\") if sst <= 0 else 1 - (np.sum((y_pred_va - y_true_va)**2) / sst)\n",
    "\n",
    "        return ae, reg, gcn, z_gcn_full, mae_tr, R2_tr, mse_tr, mae_va, R2_va, mse_va\n",
    "\n",
    "    def _score_and_train_for_S(self, S_try, genes_train_in, genes_val_in):\n",
    "        ae = self.pretrain_ae_stage1(S_try, gene_idx=genes_train_in)\n",
    "        \n",
    "        def _targets_per_gene_for_len(s_len):\n",
    "            return 1 if s_len <= 1 else 2\n",
    "            \n",
    "        ntpg = _targets_per_gene_for_len(len(S_try))\n",
    "        g_tr, t_tr = self.make_pairs(S_try, genes_train_in, use_all_targets=False, n_targets_per_gene=ntpg)\n",
    "        g_va, t_va = self.make_pairs(S_try, genes_val_in, use_all_targets=True)\n",
    "\n",
    "        reg, gcn, z_gcn_full, mae2, R22 = self.stage2_train_regressor(\n",
    "            S_try, (g_tr, t_tr), (g_va, t_va), ae\n",
    "        )\n",
    "\n",
    "        ae, reg, gcn, z_gcn_full, mae_tr, R2_tr, mse_tr, mae_va, R2_va, mse_va = self.stage3_joint_finetune(\n",
    "            S_try, (g_tr, t_tr), (g_va, t_va), ae, reg, gcn, z_gcn_full\n",
    "        )\n",
    "\n",
    "        return (ae, reg, gcn, z_gcn_full), mae_tr, R2_tr, mse_tr, mae_va, R2_va, mse_va\n",
    "\n",
    "    def beam_search_select(self, beam_width, max_ref, genes_train_in, genes_val_in, verbose=False):\n",
    "        t0 = perf_counter()\n",
    "        init_entries = []\n",
    "        for t0_idx in range(self.T):\n",
    "            S0 = [t0_idx]\n",
    "            pack0, mae_tr0, R2_tr0, mse_tr0, mae_va0, R2_va0, mse_va0 = self._score_and_train_for_S(\n",
    "                S0, genes_train_in, genes_val_in\n",
    "            )\n",
    "            init_entries.append({\n",
    "                \"S\": S0, \"pack\": pack0,\n",
    "                \"mae_tr\": mae_tr0, \"R2_tr\": R2_tr0, \"mse_tr\": mse_tr0,\n",
    "                \"mae_va\": mae_va0, \"R2_va\": R2_va0, \"mse_va\": mse_va0,\n",
    "                \"added_t\": t0_idx\n",
    "            })\n",
    "        init_entries.sort(\n",
    "            key=lambda e: (e[\"mae_tr\"], -np.nan_to_num(e[\"R2_tr\"], nan=-1e9))\n",
    "        )\n",
    "        beam = init_entries[:beam_width]\n",
    "\n",
    "        step_time = perf_counter() - t0\n",
    "        history = [{\n",
    "            \"step\": 1, \"S\": beam[0][\"S\"].copy(),\n",
    "            \"added_t\": beam[0][\"added_t\"],\n",
    "            \"TRAIN_MAE\": round(beam[0][\"mae_tr\"], 6),\n",
    "            \"TRAIN_MSE\": round(beam[0][\"mse_tr\"], 6),\n",
    "            \"TRAIN_R2\": round(beam[0][\"R2_tr\"], 6),\n",
    "            \"VAL_MAE\": round(beam[0][\"mae_va\"], 6),\n",
    "            \"VAL_MSE\": round(beam[0][\"mse_va\"], 6),\n",
    "            \"VAL_R2\": round(beam[0][\"R2_va\"], 6),\n",
    "            \"time_sec\": step_time\n",
    "        }]\n",
    "\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"[Step 1] S={history[-1]['S']} added_t={history[-1]['added_t']} \"\n",
    "                f\"MAE_train={history[-1]['TRAIN_MAE']:.6f} MSE_train={history[-1]['TRAIN_MSE']:.6f} \"\n",
    "                f\"R2_train={history[-1]['TRAIN_R2']:.6f} | \"\n",
    "                f\"MAE_val={history[-1]['VAL_MAE']:.6f} MSE_val={history[-1]['VAL_MSE']:.6f} \"\n",
    "                f\"R2_val={history[-1]['VAL_R2']:.6f} time={_fmt_secs(history[-1]['time_sec'])}\"\n",
    "            )\n",
    "\n",
    "        depth = 1\n",
    "        L_target = min(max_ref, self.T)\n",
    "        while depth < L_target:\n",
    "            depth += 1\n",
    "            d_start = perf_counter()\n",
    "            candidates = []\n",
    "            for entry in beam:\n",
    "                S_curr = entry[\"S\"]\n",
    "                remaining = [t for t in range(self.T) if t not in S_curr]\n",
    "                for t_star in remaining:\n",
    "                    S_try = S_curr + [t_star]\n",
    "                    pack, mae_tr, R2_tr, mse_tr, mae_va, R2_va, mse_va = \\\n",
    "                        self._score_and_train_for_S(S_try, genes_train_in, genes_val_in)\n",
    "                    candidates.append({\n",
    "                        \"S\": S_try, \"pack\": pack,\n",
    "                        \"mae_tr\": mae_tr, \"R2_tr\": R2_tr, \"mse_tr\": mse_tr,\n",
    "                        \"mae_va\": mae_va, \"R2_va\": R2_va, \"mse_va\": mse_va,\n",
    "                        \"added_t\": t_star\n",
    "                    })\n",
    "            candidates.sort(\n",
    "                key=lambda e: (e[\"mae_tr\"], -np.nan_to_num(e[\"R2_tr\"], nan=-1e9))\n",
    "            )\n",
    "            beam = candidates[:beam_width]\n",
    "            d_time = perf_counter() - d_start\n",
    "            top = beam[0]\n",
    "            history.append({\n",
    "                \"step\": depth,\n",
    "                \"S\": top[\"S\"].copy(),\n",
    "                \"added_t\": top[\"added_t\"],\n",
    "                \"TRAIN_MAE\": round(top[\"mae_tr\"], 6),\n",
    "                \"TRAIN_MSE\": round(top[\"mse_tr\"], 6),\n",
    "                \"TRAIN_R2\": round(top[\"R2_tr\"], 6),\n",
    "                \"VAL_MAE\": round(top[\"mae_va\"], 6),\n",
    "                \"VAL_MSE\": round(top[\"mse_va\"], 6),\n",
    "                \"VAL_R2\": round(top[\"R2_va\"], 6),\n",
    "                \"time_sec\": d_time\n",
    "            })\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"[Step {depth}] S={history[-1]['S']} added_t={history[-1]['added_t']} \"\n",
    "                    f\"MAE_train={history[-1]['TRAIN_MAE']:.6f} MSE_train={history[-1]['TRAIN_MSE']:.6f} \"\n",
    "                    f\"R2_train={history[-1]['TRAIN_R2']:.6f} | \"\n",
    "                    f\"MAE_val={history[-1]['VAL_MAE']:.6f} MSE_val={history[-1]['VAL_MSE']:.6f} \"\n",
    "                    f\"R2_val={history[-1]['VAL_R2']:.6f} time={_fmt_secs(history[-1]['time_sec'])}\"\n",
    "                )\n",
    "\n",
    "        best = min(\n",
    "            beam,\n",
    "            key=lambda e: (e[\"mae_tr\"], -np.nan_to_num(e[\"R2_tr\"], nan=-1e9))\n",
    "        )\n",
    "        return best[\"S\"], best[\"pack\"], history\n",
    "    \n",
    "    def predict_full_from_pack(self, S, pack):\n",
    "        ae, reg, gcn, z_gcn_full = pack\n",
    "        reg.eval(); ae.eval(); gcn.eval()\n",
    "        \n",
    "        T_, G_ = self.T, self.G\n",
    "        allg = np.arange(G_)\n",
    "        P = np.zeros((G_, T_), dtype=np.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for t in range(T_):\n",
    "                for i in range(0, G_, self.BATCH_G):\n",
    "                    g_slice = allg[i:i+self.BATCH_G]\n",
    "                    t_slice = np.full_like(g_slice, t)\n",
    "                    \n",
    "                    S_arr = np.array(S, dtype=np.int64)\n",
    "                    X_full = self.X_log.T.astype(np.float32)\n",
    "                    mask = np.zeros(self.T, dtype=np.float32); mask[S_arr] = 1\n",
    "                    X_mask = X_full * mask[None, :]\n",
    "\n",
    "                    gv = X_mask[g_slice].astype(np.float32)\n",
    "                    gv_t = torch.from_numpy(gv).to(self.DEVICE)\n",
    "                    \n",
    "                    z_ae = ae.enc(gv_t)\n",
    "                    \n",
    "                    if z_gcn_full is not None:\n",
    "                        z_gcn = z_gcn_full[torch.from_numpy(g_slice).to(self.DEVICE)]\n",
    "                    else:\n",
    "                        X_G_T_masked = torch.from_numpy(X_mask).to(self.DEVICE)\n",
    "                        z_all = gcn(X_G_T_masked)\n",
    "                        z_gcn = z_all[torch.from_numpy(g_slice).to(self.DEVICE)]\n",
    "\n",
    "                    pe_t_np = sinusoidal_pe(t_slice, d_model=self.PE_DIM)\n",
    "                    pe_t = torch.from_numpy(pe_t_np).to(self.DEVICE)\n",
    "\n",
    "                    xb = torch.cat([z_ae, z_gcn, pe_t], dim=1)\n",
    "                    yp = reg(xb).cpu().numpy().ravel()\n",
    "                    P[i:i+len(g_slice), t] = yp\n",
    "        return P\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_PATH = Path(\"bulk_data_lungalveoli_TPS.h5ad\")\n",
    "    try:\n",
    "        adata = ad.read_h5ad(DATA_PATH)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load {DATA_PATH}: {e}\")\n",
    "        print(\"Creating dummy data for demo...\")\n",
    "        T, G = 10, 500\n",
    "        X_dummy = np.random.randn(T, G).astype(np.float32)\n",
    "        adata = ad.AnnData(X=X_dummy)\n",
    "\n",
    "    dtpsp = DTPSP_Selector(\n",
    "        seed=1234,\n",
    "        max_ref=15,\n",
    "        beam_width=32,\n",
    "        precompute_gcn=False\n",
    "    )\n",
    "\n",
    "    print(\"Starting Beam Search...\")\n",
    "    S_sel, pack_sel, hist = dtpsp.fit(adata, normalize_data=False)\n",
    "    \n",
    "    print(\"\\nSelected S:\", S_sel)\n",
    "    \n",
    "    pred_GT = dtpsp.predict_full_from_pack(S_sel, pack_sel)\n",
    "    print(\"Prediction shape:\", pred_GT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dc3dd7-30fd-47ba-a2c2-43edf69194a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb,sys,os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "sc.settings.verbosity = 0\n",
    "import argparse\n",
    "import copy\n",
    "import numpy as np\n",
    "import scipy\n",
    "import timeit\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "import scSemiProfiler as semi\n",
    "from scSemiProfiler.utils import *\n",
    "name = 'single_cell_inference_project_lung_Alveolus_high'\n",
    "bulk = 'bulk_data_lung_Alveolus.h5ad'\n",
    "logged = False\n",
    "normed = False\n",
    "geneselection = False\n",
    "batch = 3\n",
    "\n",
    "t0 = anndata.read_h5ad(\"single_cell_inference_project_lung_Alveolus_high/sample_sc/t0.h5ad\")\n",
    "t1 = anndata.read_h5ad(\"single_cell_inference_project_lung_Alveolus_high/sample_sc/t1.h5ad\")\n",
    "t2 = anndata.read_h5ad(\"single_cell_inference_project_lung_Alveolus_high/sample_sc/t2.h5ad\")\n",
    "t3 = anndata.read_h5ad(\"single_cell_inference_project_lung_Alveolus_high/sample_sc/t3.h5ad\")\n",
    "t4 = anndata.read_h5ad(\"single_cell_inference_project_lung_Alveolus_high/sample_sc/t4.h5ad\")\n",
    "t5 = anndata.read_h5ad(\"single_cell_inference_project_lung_Alveolus_high/sample_sc/t5.h5ad\")\n",
    "t6 = anndata.read_h5ad(\"single_cell_inference_project_lung_Alveolus_high/sample_sc/t6.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879b20f-4e28-4b2b-9cee-a6940328c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb, sys, os\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "import argparse\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from typing import Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def initsetup(name: str, bulk: str, logged: bool = False, normed: bool = True,\n",
    "              geneselection: Union[bool, int] = True, representatives: list = None) -> None:\n",
    "    \"\"\"\n",
    "    Initial setup of the semi-profiling pipeline, processing the bulk data,\n",
    "    and assigning each sample to the nearest fixed representative.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name : str\n",
    "        Project name.\n",
    "    bulk : str\n",
    "        Path to bulk data as an h5ad file.\n",
    "    logged : bool\n",
    "        Whether the data has been logged or not.\n",
    "    normed : bool\n",
    "        Whether the library size has been normalized or not.\n",
    "    geneselection : bool or int\n",
    "        Perform gene selection (boolean) or specify number of highly variable genes.\n",
    "    representatives : list\n",
    "        Indices of fixed representative samples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> name = 'runexample'\n",
    "    >>> bulk = 'example_data/bulkdata.h5ad'\n",
    "    >>> logged = False\n",
    "    >>> normed = True\n",
    "    >>> geneselection = False\n",
    "    >>> representatives = [0, 2, 5]  # Fixed representative indices\n",
    "    >>> initsetup(name, bulk, logged, normed, geneselection, representatives)\n",
    "    \"\"\"\n",
    "\n",
    "    print('Start initial setup')\n",
    "\n",
    "    if not os.path.isdir(name):\n",
    "        os.system('mkdir ' + name)\n",
    "    else:\n",
    "        print(name + ' exists. Please choose another name.')\n",
    "        return\n",
    "\n",
    "    if not os.path.isdir(name + '/figures'):\n",
    "        os.system('mkdir ' + name + '/figures')\n",
    "\n",
    "    bulkdata = anndata.read_h5ad(bulk)\n",
    "\n",
    "    if not normed:\n",
    "        if logged:\n",
    "            print('Bad data preprocessing. Normalize library size before log-transformation.')\n",
    "            return\n",
    "        sc.pp.normalize_total(bulkdata, target_sum=1e4)\n",
    "\n",
    "    if not logged:\n",
    "        sc.pp.log1p(bulkdata)\n",
    "\n",
    "    sids = list(bulkdata.obs['sample_ids'])\n",
    "    with open(name + '/sids.txt', 'w') as f:\n",
    "        for sid in sids:\n",
    "            f.write(sid + '\\n')\n",
    "\n",
    "    if geneselection is False:\n",
    "        hvgenes = np.array(bulkdata.var.index)\n",
    "    elif geneselection is True:\n",
    "        sc.pp.highly_variable_genes(bulkdata, n_top_genes=6000)\n",
    "        bulkdata = bulkdata[:, bulkdata.var.highly_variable]\n",
    "        hvgenes = np.array(bulkdata.var.index)[bulkdata.var.highly_variable]\n",
    "    else:\n",
    "        sc.pp.highly_variable_genes(bulkdata, n_top_genes=int(geneselection))\n",
    "        bulkdata = bulkdata[:, bulkdata.var.highly_variable]\n",
    "        hvgenes = np.array(bulkdata.var.index)[bulkdata.var.highly_variable]\n",
    "    np.save(name + '/hvgenes.npy', hvgenes)\n",
    "\n",
    "    n_comps = min(100, bulkdata.X.shape[0] - 1)\n",
    "    sc.tl.pca(bulkdata, n_comps=n_comps)\n",
    "\n",
    "    bulkdata.write(name + '/processed_bulkdata.h5ad')\n",
    "\n",
    "    if representatives is None or len(representatives) == 0:\n",
    "        print(\"Please provide fixed representative indices.\")\n",
    "        return\n",
    "\n",
    "    representatives_pca = bulkdata.obsm['X_pca'][representatives]\n",
    "    distances = pairwise_distances(bulkdata.obsm['X_pca'], representatives_pca)\n",
    "    cluster_labels = np.argmin(distances, axis=1)\n",
    "\n",
    "    # Store the cluster labels\n",
    "    if not os.path.isdir(name + '/status'):\n",
    "        os.system('mkdir ' + name + '/status')\n",
    "\n",
    "    with open(name + '/status/init_cluster_labels.txt', 'w') as f:\n",
    "        for label in cluster_labels:\n",
    "            f.write(str(label) + '\\n')\n",
    "\n",
    "    with open(name + '/status/init_representatives.txt', 'w') as f:\n",
    "        for rep in representatives:\n",
    "            f.write(str(rep) + '\\n')\n",
    "\n",
    "    print('Initial setup finished. Among ' + str(len(sids)) +\n",
    "          ' total samples, assigned to fixed representatives:')\n",
    "    for i, rep in enumerate(representatives):\n",
    "        print(f\"Cluster {i} representative: {sids[rep]}\")\n",
    "\n",
    "    return\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c8ec2f-4bd0-4870-95da-2bacbd885284",
   "metadata": {},
   "outputs": [],
   "source": [
    "initsetup(name,bulk,logged=logged,normed=normed,geneselection=True,representatives=[0,3,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14861dfc-508e-40fa-a442-ff6f71af519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import hdf5plugin\n",
    "\n",
    "reps_processed = ad.concat([t0, t3,t6], axis=0, join='inner')\n",
    "\n",
    "print(f\"Number of observations (cells): {reps_processed.n_obs}\")\n",
    "print(f\"Number of variables (genes): {reps_processed.n_vars}\")\n",
    "\n",
    "if 'cell_id' not in reps_processed.obs.columns:\n",
    "    reps_processed.obs['cell_id'] = reps_processed.obs_names\n",
    "\n",
    "if 'n_genes' not in reps_processed.obs.columns:\n",
    "    reps_processed.obs['n_genes'] = (reps_processed.X > 0).sum(axis=1)\n",
    "\n",
    "\n",
    "if 'gene_ids' not in reps_processed.var.columns:\n",
    "    reps_processed.var['gene_ids'] = reps_processed.var_names\n",
    "\n",
    "\n",
    "reps_processed.obs.columns = reps_processed.obs.columns.astype(str)\n",
    "reps_processed.var.columns = reps_processed.var.columns.astype(str)\n",
    "\n",
    "# Convert object dtype columns in obs and var to strings\n",
    "for col in reps_processed.obs.columns:\n",
    "    if reps_processed.obs[col].dtype == 'object':\n",
    "        reps_processed.obs[col] = reps_processed.obs[col].astype(str)\n",
    "\n",
    "for col in reps_processed.var.columns:\n",
    "    if reps_processed.var[col].dtype == 'object':\n",
    "        reps_processed.var[col] = reps_processed.var[col].astype(str)\n",
    "\n",
    "print(\"Data types in obs:\")\n",
    "print(reps_processed.obs.dtypes)\n",
    "print(\"Data types in var:\")\n",
    "print(reps_processed.var.dtypes)\n",
    "import numpy as np\n",
    "\n",
    "hvgenes = np.load(name + '/hvgenes.npy', allow_pickle=True)\n",
    "\n",
    "print(\"First few genes in hvgenes:\", hvgenes[:5])\n",
    "\n",
    "reps_genes = reps_processed.var_names\n",
    "\n",
    "common_genes = np.intersect1d(hvgenes, reps_genes)\n",
    "\n",
    "print(f\"Number of genes in hvgenes: {len(hvgenes)}\")\n",
    "print(f\"Number of genes in reps_processed: {len(reps_genes)}\")\n",
    "print(f\"Number of common genes: {len(common_genes)}\")\n",
    "\n",
    "missing_in_reps = np.setdiff1d(hvgenes, reps_genes)\n",
    "print(f\"Number of genes in hvgenes not in reps_processed: {len(missing_in_reps)}\")\n",
    "\n",
    "hvgenes_in_reps_ordered = [gene for gene in hvgenes if gene in reps_genes]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reps_filtered = reps_processed[:, hvgenes_in_reps_ordered].copy()\n",
    "\n",
    "\n",
    "assert all(reps_filtered.var_names == hvgenes_in_reps_ordered), \"Gene order does not match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f9d82f-c3c7-4efe-9326-8ea7d5b3807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reps_filtered.write_h5ad(\n",
    "      name+'/representative_sc.h5ad',\n",
    "      compression=hdf5plugin.FILTERS[\"zstd\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0a255-a5bb-4149-ac32-2917f98736a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "semi.scprocess(name=name,singlecell=name+'/representative_sc.h5ad',normed=True,logged=False,cellfilter=False,threshold=1e-3,geneset=True,weight=0.5,k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d4c696-ff53-4786-9bfb-b649e50c1924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the representatives and clusterings\n",
    "sids = []\n",
    "f = open(name + '/sids.txt','r')\n",
    "lines = f.readlines()\n",
    "for l in lines:\n",
    "    sids.append(l.strip())\n",
    "f.close()\n",
    "\n",
    "repres = []\n",
    "f=open(name + '/status/init_representatives.txt','r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "for l in lines:\n",
    "    repres.append(int(l.strip()))\n",
    "\n",
    "cl = []\n",
    "f=open(name + '/status/init_cluster_labels.txt','r')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "for l in lines:\n",
    "    cl.append(int(l.strip()))\n",
    "\n",
    "print('representatives:',repres)\n",
    "print('cluster labels:',cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7330a8bb-7e69-4c55-9425-499e5f91873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "representatives = name + '/status/init_representatives.txt'\n",
    "cluster = name + '/status/init_cluster_labels.txt'\n",
    "\n",
    "bulktype = 'pseudobulk'\n",
    "semi.scinfer(name, representatives,cluster,bulktype, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab331ab7-96f5-4dce-85de-198dd63740f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = cl\n",
    "semisdata = assemble_cohort(name,\n",
    "                repres,\n",
    "                cl,\n",
    "                celltype_key = 'celltype',\n",
    "                sample_info_keys = ['sample_ids'],\n",
    "                bulkpath= 'bulk_data_lung_Alveolus.h5ad')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74eec38-ba9f-4b9d-b2e6-4b0744b07391",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read the combined adata of gound true single cell data for subsequent comparison\n",
    "combined_adata = anndata.read_h5ad(name+\"/combined_data.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74224b11-3a7c-4f97-ba82-bfe3f873b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out NA celltypes\n",
    "import pandas as pd\n",
    "\n",
    "invalid_values = [None, pd.NA, float('nan'), 'nan', 'NA']\n",
    "\n",
    "def filter_invalid_celltypes(adata):\n",
    "    return adata[~adata.obs['celltype'].astype(str).str.strip().isin(invalid_values)].copy()\n",
    "\n",
    "combined_adata = filter_invalid_celltypes(combined_adata)\n",
    "semisdata = filter_invalid_celltypes(semisdata)\n",
    "\n",
    "print(f\"Filtered combined_adata cells: {combined_adata.n_obs}\")\n",
    "print(f\"Filtered semisdata cells: {semisdata.n_obs}\")\n",
    "\n",
    "combined_adata.write_h5ad('combined_adata_filtered.h5ad')\n",
    "semisdata.write_h5ad('semisdata_filtered.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3ec175-e281-406c-a0e0-58382ebe0fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# visualize distribution of assembled ground truth data and semi-profiled data\n",
    "combined_data,gtdata,semidata = compare_umaps(\n",
    "            semidata = semisdata,\n",
    "            gtdata = combined_adata,\n",
    "            name = name,\n",
    "            representatives = name + '/status/init_representatives.txt',\n",
    "            cluster_labels = name + '/status/init_cluster_labels.txt',\n",
    "            celltype_key = 'celltype',\n",
    "            save = name+\"/figures\"\n",
    "            )\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee9eb87-0d40-42a4-b2e7-56348a9410ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def composition_by_group(\n",
    "    adata: anndata.AnnData,\n",
    "    colormap: Union[str, list] = None,\n",
    "    groupby: str = None,\n",
    "    title: str = 'Cell type composition',\n",
    "    save: str = None,\n",
    "    name: str = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualizing the cell type composition in each group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata:\n",
    "        The dataset to investigate.\n",
    "    colormap:\n",
    "        The colormap for visualization.\n",
    "    groupby:\n",
    "        The key in .obs specifying groups.\n",
    "    title:\n",
    "        Plot title.\n",
    "    save:\n",
    "        Path to save the plot as a PDF file.\n",
    "    name:\n",
    "        Folder to save the file in, if provided.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        None\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> groupby = 'states_collection_sum'\n",
    "    >>> composition_by_group2(\n",
    "    >>>     adata=gtdata,\n",
    "    >>>     groupby=groupby,\n",
    "    >>>     title='Ground truth'\n",
    "    >>> )\n",
    "    \"\"\"\n",
    "    totaltypes = np.array(adata.obs['celltype'].cat.categories)\n",
    "\n",
    "    if colormap is None:\n",
    "        colormap = adata.uns['celltypes_colors']\n",
    "\n",
    "    conditions = np.unique(adata.obs[groupby])\n",
    "    n = conditions.shape[0]\n",
    "    percentages = []\n",
    "\n",
    "    for i in range(conditions.shape[0]):\n",
    "        condition_prop = celltype_proportion(adata[adata.obs[groupby] == conditions[i]], totaltypes)\n",
    "        percentages.append(condition_prop)\n",
    "\n",
    "    fig, axs = plt.subplots(n, 1, figsize=(n, 1))\n",
    "    axs[0].set_title(title)\n",
    "\n",
    "    for j in range(n):\n",
    "        for i in range(len(totaltypes)):\n",
    "            axs[j].barh(conditions[j], percentages[j][i], left=sum(percentages[j][:i]), color=colormap[i])\n",
    "            axs[j].set_xlim([0, 1])\n",
    "            axs[j].set_yticklabels([])\n",
    "            axs[j].yaxis.set_tick_params(left=False, right=False, labelleft=False, labelbottom=False, bottom=False)\n",
    "\n",
    "            if j != n:\n",
    "                axs[j].set_xticklabels([])\n",
    "\n",
    "        axs[j].text(-0.01, 0, conditions[j], ha='right', va='center')\n",
    "\n",
    "    patches = [mpatches.Patch(color=colormap[i], label=totaltypes[i]) for i in range(len(totaltypes))]\n",
    "    axs[-1].legend(handles=patches, loc='center left', bbox_to_anchor=(1.1, n))\n",
    "\n",
    "    plt.xlabel('Proportion')\n",
    "\n",
    "    if save is not None:\n",
    "        save_path = f\"{name}/{save}.pdf\" if name else f\"{save}.pdf\"\n",
    "        plt.savefig(save_path, format='pdf', dpi=600, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ab503-2f48-4238-ba99-610073957f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize cell types composition by timepoints\n",
    "groupby = 'sample_ids'\n",
    "composition_by_group(\n",
    "    adata = combined_adata,\n",
    "    groupby = groupby,\n",
    "    title = 'Ground truth',\n",
    "    colormap = semidata.uns['celltype_colors'],\n",
    "    save = \"/composition_gt\",\n",
    "    name = name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb7757f-f201-405b-87e4-ba3a014b248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichment_comparison(name, combined_adata, semisdata, celltype_key = 'celltype', selectedtype = \"AT1\", save = \"figures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d8401-5978-4519-b21f-f6311ed9282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrichment_comparison_reactome(name:str,\n",
    "                                   gtdata:anndata.AnnData,\n",
    "                                   semisdata:anndata.AnnData,\n",
    "                                   celltype_key:str,\n",
    "                                   selectedtype:str,\n",
    "                                   save = None\n",
    "                                  ) -> Tuple[np.array, np.array, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Compare the enrichment analysis results using the real-profiled and semi-profiled datasets, using Reactome pathway sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    name:\n",
    "        Project name\n",
    "    gtdata:\n",
    "        Real-profiled (ground truth) data (AnnData object)\n",
    "    semisdata:\n",
    "        Semi-profiled dataset (AnnData object)\n",
    "    celltype_key:\n",
    "        The key in anndata.AnnData.obs that stores cell type information\n",
    "    selectedtype:\n",
    "        The selected cell type to analyze\n",
    "    save:\n",
    "        Path within the 'figures' folder to save the plot\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    CommonDEGs : int\n",
    "        The number of overlapping DEGs between real and semi-profiled data\n",
    "    HypergeometricP : float\n",
    "        P-value of hypergeometric test examining the overlap between two versions of DEGs\n",
    "    PearsonR : float\n",
    "        Pearson correlation between bar lengths in real-profiled and semi-profiled bar plots\n",
    "    PearsonP : float\n",
    "        P-value of the Pearson correlation test\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> _ = enrichment_comparison_reactome(name, gtdata, semisdata, celltype_key='celltypes', selectedtype='CD4')\n",
    "    \"\"\"\n",
    "\n",
    "    totaltypes = np.unique(gtdata.obs[celltype_key])\n",
    "\n",
    "    sc.tl.rank_genes_groups(gtdata, celltype_key, method='t-test')\n",
    "    typededic = {}\n",
    "    for j in range(totaltypes.shape[0]):\n",
    "        celltype = totaltypes[j]\n",
    "        typede = []\n",
    "        for i in range(100):\n",
    "            g = gtdata.uns['rank_genes_groups']['names'][i][j]\n",
    "            typede.append(g)\n",
    "        typededic[celltype] = typede\n",
    "\n",
    "    sc.tl.rank_genes_groups(semisdata, celltype_key, method='t-test')\n",
    "    semitypededic = {}\n",
    "    for j in range(totaltypes.shape[0]):\n",
    "        celltype = totaltypes[j]\n",
    "        typede = []\n",
    "        for i in range(100):\n",
    "            g = semisdata.uns['rank_genes_groups']['names'][i][j]\n",
    "            typede.append(g)\n",
    "        semitypededic[celltype] = typede\n",
    "\n",
    "    gtdeg = typededic[selectedtype]\n",
    "    semideg = semitypededic[selectedtype]\n",
    "    c = sum([1 for i in semideg if i in gtdeg])\n",
    "\n",
    "    hyperpval = hypert(semisdata.X.shape[1], 100, 100, c)\n",
    "    print('p-value of hypergeometric test for overlapping DEGs:', str(float(hyperpval)))\n",
    "\n",
    "    if (os.path.isdir(name + '/gseapygt')) == False:\n",
    "        os.system('mkdir ' + name + '/gseapygt')\n",
    "    if (os.path.isdir(name + '/gseapysemi')) == False:\n",
    "        os.system('mkdir ' + name + '/gseapysemi')\n",
    "\n",
    "    results = gseapy.enrichr(gene_list=gtdeg, gene_sets='Reactome_2022', outdir=name + '/gseapygt')\n",
    "    f = open(name + '/gseapygt/Reactome_2022.human.enrichr.reports.txt', 'r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    gtsets = []\n",
    "    gtps = []\n",
    "    gtdic = {}\n",
    "    for l in lines[1:]:\n",
    "        term = l.split('\\t')[1]\n",
    "        p = float(l.split('\\t')[4])\n",
    "        gtsets.append(term)\n",
    "        gtps.append(p)\n",
    "        gtdic[term] = p\n",
    "\n",
    "    results = gseapy.enrichr(gene_list=semideg, gene_sets='Reactome_2022', outdir=name + '/gseapysemi')\n",
    "    f = open(name + '/gseapysemi/Reactome_2022.human.enrichr.reports.txt','r')\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    semisets = []\n",
    "    semips = []\n",
    "    semidic = {}\n",
    "    for l in lines[1:]:\n",
    "        term = l.split('\\t')[1]\n",
    "        p = float(l.split('\\t')[4])\n",
    "        semisets.append(term)\n",
    "        semips.append(p)\n",
    "        semidic[term] = p\n",
    "\n",
    "    terms = copy.deepcopy(gtsets[:10])\n",
    "    real_data = copy.deepcopy(gtps[:10])\n",
    "    sim_data = []\n",
    "    for i in range(10):\n",
    "        gtterm = semisets[i]\n",
    "        if gtterm not in semidic.keys():\n",
    "            sim_data.append(1)\n",
    "        else:\n",
    "            sim_data.append(semidic[gtterm])\n",
    "\n",
    "    for i in range(10):\n",
    "        if semisets[i] in terms:\n",
    "            continue\n",
    "        terms.append(semisets[i])\n",
    "        sim_data.append(semips[i])\n",
    "        if semisets[i] not in gtdic.keys():\n",
    "            real_data.append(1)\n",
    "        else:\n",
    "            real_data.append(gtdic[semisets[i]])\n",
    "\n",
    "    real_data = np.flip(real_data)\n",
    "    sim_data = np.flip(sim_data)\n",
    "    terms = np.flip(terms)\n",
    "    sim_bar_lengths = [-np.log10(p) for p in sim_data]\n",
    "    real_bar_lengths = [-np.log10(p) for p in real_data]\n",
    "\n",
    "    res = scipy.stats.pearsonr(np.array(sim_bar_lengths), np.array(real_bar_lengths))\n",
    "    print('Significance correlation:', res)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(8, 5))\n",
    "    bar_width = 0.4\n",
    "    y = np.arange(len(sim_data)) + 1\n",
    "    ax1.barh(y, real_bar_lengths, height=bar_width, color='green', label='Real')\n",
    "    ax1.set_xlabel('-log10(p)')\n",
    "    ax1.set_ylabel('Term')\n",
    "    ax1.set_title('Real Data (' + str(len(semideg)) + ' DEGs)')\n",
    "    ax2.barh(y, sim_bar_lengths, height=bar_width, color='blue', label='Simulated')\n",
    "    ax2.set_xlabel('-log10(p)')\n",
    "    ax2.set_title('Semi-profiled Data(' + str(len(gtdeg)) + ' DEGs)')\n",
    "\n",
    "    max_val = max(max(sim_bar_lengths), max(real_bar_lengths))\n",
    "    ax1.set_xlim(0, max_val + 1)\n",
    "    ax2.set_xlim(0, max_val + 1)\n",
    "    ax1.invert_xaxis()\n",
    "    ax1.set_yticks(y)\n",
    "    ax2.set_yticklabels(terms)\n",
    "    fig.suptitle(selectedtype + ' Reactome (' + str(c) + ' Overlap DEGs)')\n",
    "\n",
    "    if save is not None:\n",
    "        plt.savefig(name + '/figures/' + save + selectedtype + ' Reactome.pdf', bbox_inches='tight')\n",
    "        plt.savefig(name + '/figures/' + save + selectedtype + ' Reactome.jpg', dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return c, float(hyperpval), res[0], res[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c101d0b3-364e-40ff-84b5-bf1d219374a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enrichment_comparison_reactome(name, combined_adata, semisdata, celltype_key = 'celltype', selectedtype = \"AT1\", save = \"figures\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
